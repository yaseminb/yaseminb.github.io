<!DOCTYPE html>
<html lang="en">
<body bgcolor="#D7D9E3">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.3.0 -->
<title>Research | Yasemin Bekiroglu</title>
<meta property="og:title" content="Research" />
<meta property="og:locale" content="en_US" />
<meta property="og:site_name" content="Yasemin Bekiroglu" />
<script type="application/ld+json">
{"name":null,"description":null,"author":null,"@type":"WebPage","url":"/research/","publisher":null,"image":null,"headline":"Publications","dateModified":null,"datePublished":null,"sameAs":null,"mainEntityOfPage":null,"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="alternate" type="application/rss+xml" title="Yasemin Bekiroglu" href="/feed.xml">
  <!--link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
     <link rel="stylesheet" href="/css/academicons.min.css"/>
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" rel="author" href="/">Yasemin Bekiro&#287;lu</a>

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
             <i class="fa fa-search" style="color:gray" ></i><a class="page-link" href="/research/">Research</a></i>

            <i class="fa fa-folder-open" style="color:gray" ></i><a class="page-link" href="/publications/">Publications</a></i>
            
          
            
            
            <i class="fa fa-wrench" style="color:gray" ></i><a class="page-link" href="/workshops/">Workshops</a></i>
            
           <i class="fa fa-database" style="color:gray"></i><a class="page-link" href="/data/">Data</a></i>
            
            
            <i class="fa fa-puzzle-piece" style="color:gray"></i><a class="page-link" href="/misc/">Misc</a></i>
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <!--h3 class="post-title">Publications</h3-->
    <!--h3 class="post-title"><b><font color=0B1D76>Publications</font></b></h3-->
    <h2><b><font color=0B1D76>Research</font></b></h2>
  </header>

  <div class="post-content">
    <!--p><b></b></p-->
<p align="justify">
<!--My research field is in the intersection of robotics, machine learning and computer vision. -->
My research is focused on data-efficient learning from multisensory data, e.g tactile, visual and proprioceptive, to equip robots with dexterity and high level reasoning required for achieving complex tasks autonomously. In particular, I am interested in studying how to better integrate touch sensing for robotic grasping and manipulation. Some research directions and relevant selected papers are listed below.

    </p>

    <br>

    <p><b>Multimodal (e.g. tactile, visual, proprioceptive) sensing for grasp analysis</b></p>
<li>
<p><img style="padding: 0 15px; float: left" height="140" src="../images/bn.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px" align="justify"></p>

    <p align="justify">A Probabilistic Framework for Task-Oriented Grasp Stability Assessment, 
      IEEE International Conference on Robotics and Automation, 2013</p>
      <p align="justify">
      We present a probabilistic framework for grasp modeling and stability assessment. The framework facilitates assessment of grasp success in a goal-oriented way, taking into account both geometric constraints for task affordances and stability requirements specific for a task. We integrate high-level task information introduced by a teacher in a supervised setting with low-level stability requirements acquired through a robot's self-exploration. The conditional relations between tasks and multiple sensory streams (vision, proprioception and tactile) are modeled using Bayesian networks. The generative modeling approach both allows prediction of grasp success, and provides insights into dependencies between variables and features relevant for object grasping.
      </p>
  </li>
  <li>
    <p><img style="padding: 0 15px; float: left" height="140" src="../images/stability.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px" align="justify"></p>

 <p align="justify">Assessing grasp stability based on learning and haptic data, IEEE Transactions on Robotics, 2011</p>
    <p align="justify">
    An important ability of a robot that interacts with the environment and manipulates objects is to deal with the uncertainty in sensory data. Sensory information is necessary to, for example, perform online assessment of grasp stability. We present methods to assess grasp stability based on haptic data and machine-learning methods, including AdaBoost, support vector machines (SVMs), and hidden Markov models (HMMs). In particular, we study the effect of different sensory streams to grasp stability. This includes object information such as shape; grasp information such as approach vector; tactile measurements from fingertips; and joint configuration of the hand. Sensory knowledge affects the success of the grasping process both in the planning stage (before a grasp is executed) and during the execution of the grasp (closed-loop online control). In this paper, we study both of these aspects. We propose a probabilistic learning framework to assess grasp stability and demonstrate that knowledge about grasp stability can be inferred using information from tactile sensors. Experiments on both simulated and real data are shown. The results indicate that the idea to exploit the learning approach is applicable in realistic scenarios, which opens a number of interesting venues for the future research.</p>
  </li>
  <!--li>
    <p>Learning Tactile Characterizations Of Object- And Pose-specific Grasps, 
      IEEE/RSJ International Conference on Intelligent Robots and Systems, 2011</p>
  </li>
  <li>
    <p>Integrating Grasp Planning with Online Stability Assessment using Tactile Sensing, 
      IEEE International Conference on Robotics and Automation, 2011
</p>
  </li>
  <li>
    <p>Learning grasp stability with tactile data and HMMs, 
      IEEE International Symposium on Robot and Human Interactive Communication, 2010</p>
  </li-->
  <li>
    <p><img style="padding: 0 15px; float: left" height="180" src="../images/analytic.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px"  align="justify"></p>
  
 <p align="justify">
  Analytic Grasp Success Prediction with Tactile Feedback, 
      IEEE International Conference on Robotics and Automation, 2016</p>
      <p align="justify">
      Predicting grasp success is useful for avoiding failures in many robotic applications. Based on reasoning in wrench space, we address the question of how well analytic grasp success prediction works if tactile feedback is incorporated. Tactile information can alleviate contact placement uncertainties and facilitates contact modeling. We introduce a wrench-based classifier and evaluate it on a large set of real grasps. The key finding of this work is that exploiting tactile information allows wrench-based reasoning to perform on a level with existing methods based on learning or simulation. Different from these methods, the suggested approach has no need for training data, requires little modeling effort and is computationally efficient. Furthermore, our method affords task generalization by considering the capabilities of the grasping device and expected disturbance forces/moments in a physically meaningful way.</p>
  </li>
  
    <!--p align="justify">
      <li>
Grasp Quality Evaluation Done Right: How Assumed Contact Force Bounds Affect Wrench-Based Quality Metrics, 
      IEEE International Conference on Robotics and Automation, 2017</p>
      <p align="justify">
      Wrench-based quality metrics play an important role in many applications such as grasp planning or grasp success prediction. In this work, we study the following discrepancy which is frequently overlooked in practice: the quality metrics are commonly computed under the assumption of sum-magnitude bounded contact forces, but the corresponding grasps are executed by a fully actuated device where the contact forces are limited independently. By means of experiments carried out in simulation and on real hardware, we show that in this setting the values of these metrics are severely underestimated. This can lead to erroneous conclusions regarding the actual capabilities of the grasps under consideration. Our findings highlight the importance of matching the physical properties of the task and the grasping device with the chosen quality metrics.</p>
    </li-->
  <!--/li>

    <p>Evaluating the Quality of Non-Prehensile Balancing Grasps, 
      IEEE International Conference on Robotics and Automation, 2018</p>
  </li-->

    




<p><b>Grasp and manipulation planning</b></p>
<p align="justify">

  <li>
     <p><img style="padding: 0 15px; float: left" height="130" src="../images/dynamicg.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px"  align="justify"></p>

  
 <p align="justify">
Dynamic grasp and trajectory planning for moving objects, Autonomous Robots, 2019</p>
    <p align="justify">
    This paper shows how a robot arm can follow and grasp moving objects tracked by a vision system, as is needed when a human hands over an object to the robot during collaborative working. While the object is being arbitrarily moved by the human co-worker, a set of likely grasps, generated by a learned grasp planner, are evaluated online to generate a feasible grasp with respect to both: the current configuration of the robot respecting the target grasp; and the constraints of finding a collision-free trajectory to reach that configuration. A task-based cost function enables relaxation of motion-planning constraints, enabling the robot to continue following the object by maintaining its end-effector near to a likely pre-grasp position throughout the object’s motion. We propose a method of dynamic switching between: a local planner, where the hand smoothly tracks the object, maintaining a steady relative pre-grasp pose; and a global planner, which rapidly moves the hand to a new grasp on a completely different part of the object, if the previous graspable part becomes unreachable. Various experiments are conducted using a real collaborative robot and the obtained results are discussed.</p>
  </li>
 <li>
     <p><img style="padding: 0 15px; float: left" height="210" src="../images/benchmark_.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px"  align="justify"></p>

  
 <p align="justify">
  Benchmarking Protocol for Grasp Planning Algorithms, IEEE Robotics and Automation Letters, 2020
</p>
<p align="justify">
Numerous grasp planning algorithms have been proposed since the 1980s. The grasping literature has expanded rapidly in recent years, building on greatly improved vision systems and computing power. Methods have been proposed to plan stable grasps on known objects (exact 3D model is available), familiar objects (e.g. exploiting a-priori known grasps for different objects of the same category), or novel object shapes observed during task execution. Few of these methods have ever been compared in a systematic way, and objective performance evaluation of such complex systems remains problematic. Difficulties and confounding factors include different assumptions and amounts of a-priori knowledge in different algorithms; different robots, hands, vision systems and setups in different labs; and different choices or application needs for grasped objects. Also, grasp planning can use different grasp quality metrics (including empirical or theoretical stability measures) or other criteria, e.g., computational speed, or combination of grasps with reachability considerations. While acknowledging and discussing the outstanding difficulties surrounding this complex topic, we propose a methodology for reproducible experiments to compare the performance of a variety of grasp planning algorithms. Our protocol attempts to improve the objectivity with which different grasp planners are compared by minimizing the influence of key components in the grasping pipeline, e.g., vision and pose estimation. The protocol is demonstrated by evaluating two different grasp planners: a state-of-the-art model-free planner and a popular open-source model-based planner. We show results from real-robot experiments with a 7-DoF arm and 2-finger hand, as well as simulation-based evaluations.</p>
  </li>
   <li>
    <p><img style="padding: 0 15px; float: left" height="140" src="../images/gms.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px" align="justify"></p>

 <p align="justify">
  Grasp Moduli Spaces and Spherical Harmonics, IEEE International Conference on Robotics and Automation, 2014
</p>
<p align="justify">
In this work, we present a novel representation which enables a robot to reason about, transfer and optimize grasps on various objects by representing objects and grasps on them jointly in a common space. In our approach, objects are parametrized using smooth differentiable functions which are obtained from point cloud data via a spectral analysis. We show how, starting with point cloud data of various objects, one can utilize this space consisting of grasps and smooth surfaces in order to continuously deform various surface/grasp configurations with the goal of synthesizing force closed grasps on novel objects. We illustrate the resulting shape space for a collection of real world objects using multidimensional scaling and show that our formulation naturally enables us to use gradient ascent approaches to optimize and simultaneously deform a grasp from a known object towards a novel object.</p>
 </li>
  <!--/li>
 
    <p>Predicting Slippage and Learning Manipulation Affordances through Gaussian Process Regression, 
      IEEE-RAS International Conference on Humanoid Robots, 2013
</p>
  </li>
   <li>
    <p>Learning Predictive State Representation for In-Hand Manipulation, 
      IEEE International Conference on Robotics and Automation, 2015</p>
  </li-->


<p><b>Grasp adaptation</b></p>
<p align="justify">
 <li>
    <p><img style="padding: 0 15px; float: left" height="140" src="../images/objectlevelimpedance.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px" align="justify"></p>

  
 <p align="justify">
  Learning of Grasp Adaptation through Experience and Tactile Sensing, 
      IEEE/RSJ International Conference on Intelligent Robots and Systems, 2014</p>
      <p align="justify">
      To perform robust grasping, a multi-fingered robotic hand should be able to adapt its grasping configuration, i.e., how the object is grasped, to maintain the stability of the grasp. Such a change of grasp configuration is called grasp adaptation and it depends on the controller, the employed sensory feedback and the type of uncertainties inherit to the problem. This paper proposes a grasp adaptation strategy to deal with uncertainties about physical properties of objects, such as the object weight and the friction at the contact points. Based on an object-level impedance controller, a grasp stability estimator is first learned in the object frame. Once a grasp is predicted to be unstable by the stability estimator, a grasp adaptation strategy is triggered according to the similarity between the new grasp and the training examples. Experimental results demonstrate that our method improves the grasping performance on novel objects with different physical properties from those used for training.
    </p>
  </li>
 <li>
<p><img style="padding: 0 15px; float: left" height="140" src="../images/mrd.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px" align="justify"></p>
  
 <p align="justify">
  Probabilistic Consolidation of Grasp Experience, IEEE International Conference on Robotics and Automation, 2016

</p>
<p align="justify">
We present a probabilistic model for joint representation of several sensory modalities and action parameters in a robotic grasping scenario. Our non-linear probabilistic latent variable model encodes relationships between grasp-related parameters, learns the importance of features, and expresses confidence in estimates. The model learns associations between stable and unstable grasps that it experiences during an exploration phase. We demonstrate the applicability of the model for estimating grasp stability, correcting grasps, identifying objects based on tactile imprints and predicting tactile imprints from object-relative gripper poses. We performed experiments on a real platform with both known and novel objects, i.e., objects the robot trained with, and previously unseen objects. Grasp correction had a 75% success rate on known objects, and 73% on new objects. We compared our model to a traditional regression model that succeeded in correcting grasps in only 38% of cases.
</p>
 </li>
  <!--/li>
  
    <p>Hierarchical Fingertip Space: A Unified Framework for Grasp Planning and In-Hand Grasp Adaptation, 
      IEEE Transactions on Robotics, 2016
</p>
  </li-->
  

<p><b>Object modelling and scene understanding</b></p>
<p align="justify">
  <li>
    <p><img style="padding: 0 15px; float: left" height="160" src="../images/gpis_system.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px"  align="justify"></p>
  
 <p align="justify">
  Enhancing Visual Perception of Shape through Tactile Glances, 
      IEEE/RSJ International Conference on Intelligent Robots and Systems, 2013</p>
      <p align="justify">
      Object shape information is an important parameter in robot grasping tasks. However, it may be difficult to obtain accurate models of novel objects due to incomplete and noisy sensory measurements. In addition, object shape may change due to frequent interaction with the object (cereal boxes, etc). In this paper, we present a probabilistic approach for learning object models based on visual and tactile perception through physical interaction with an object. Our robot explores unknown objects by touching them strategically at parts that are uncertain in terms of shape. The robot starts by using only visual features to form an initial hypothesis about the object shape, then gradually adds tactile measurements to refine the object model. Our experiments involve ten objects of varying shapes and sizes in a real setup. The results show that our method is capable of choosing a small number of touches to construct object models similar to real object shapes and to determine similarities among acquired models.</p>
  </li>
  <li>
    <p><img style="padding: 0 15px; float: left" height="160" src="../images/spgis.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px" align="justify"></p>
  
 <p align="justify">
  Object shape estimation and modeling, based on sparse Gaussian process implicit surfaces, 
      combining visual data and tactile exploration, 
      Robotics and Autonomous Systems, 2020</p>
      <p align="justify">
      We study Gaussian Process Implicit Surface (GPIS) representation. GPIS enables a non-parametric probabilistic reconstruction of object surfaces from 3D data points, while also providing a principled approach to encode the uncertainty associated with each region of the reconstruction. We investigate different configurations for GPIS, and interpret an object surface as the level-set of an underlying sparse GP. Experiments are performed on both synthetic data, and also real data sets obtained from two different robots physically interacting with objects. We evaluate performance by assessing how close the reconstructed surfaces are to ground-truth object models. We also evaluate how well objects from different categories are clustered, based on the reconstructed surface shapes. Results show that sparse GPs enable a reliable approximation to the full GP solution, and the proposed method yields adequate surface representations to distinguish objects. Additionally the presented approach is shown to provide computational efficiency, and also efficient use of the robot’s exploratory actions.</p>
  </li>
 <!--li>
    <p>Shape Modeling based on Sparse Gaussian Process Implicit Surfaces, NeurIPS 2018, WIML workshop
</p>
  </li>
   <li>
    <p>Active Exploration Using Gaussian Random Fields and Gaussian Process Implicit Surfaces, 
      IEEE/RSJ International Conference on Intelligent Robots and Systems, 2016
</p>
  </li-->
  
  <li>
    <p><img style="padding: 0 15px; float: left" height="160" src="../images/pbp.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px" align="justify"></p>
  
 <p align="justify">
  Learning to Disambiguate Object Hypotheses through Self-Exploration, 
      IEEE-RAS International Conference on Humanoid Robots, 2014
</p>
<p align="justify">
We present a probabilistic learning framework to form object hypotheses through interaction with the environment. A robot learns how to manipulate objects through pushing actions to identify how many objects are present in the scene. We use a segmentation system that initializes object hypotheses based on RGBD data and adopt a reinforcement approach to learn the relations between pushing actions and their effects on object segmentations. Trained models are used to generate actions that result in minimum number of pushes on object groups, until either object separation events are observed or it is ensured that there is only one object acted on. We provide baseline experiments that show that a policy based on reinforcement learning for action selection results in fewer pushes, than if pushing actions were selected randomly.</p>
  </li>
  <!--li>
    <p>What's in the Container? Classifying Object Contents from Vision and Touch, 
      IEEE/RSJ International Conference on Intelligent Robots and Systems, 2014</p>
  </li>
  <li>
    <p>A Database for Reproducible Manipulation Research: CapriDB - Capture, Print, Innovate, Data in Brief, 2017
</p>
  </li-->
     </p>




<p><b>Applications (Assembly, Nuclear Decommissioning, Service)</b></p>
<p align="justify">
  <li>
     <p><img style="padding: 0 15px; float: left" height="140" src="../images/faim.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px" align="justify"></p>
  
 <p align="justify">
  Teaching Assembly by Demonstration using Advanced Human Robot Interaction and a Knowledge Integration Framework, 
      International Conference on Flexible Automation and Intelligent Manufacturing, 2017
</p>
<p align="justify">
Conventional industrial robots are heavily dependent on hard automation that requires pre-specified fixtures and time-consuming (re)programming performed by experienced operators. In this work, teaching by human-only demonstration is used for reducing required time and expertise to setup a robotized assembly station. This is achieved by the proposed framework enhancing the robotic system with advanced perception and cognitive abilities, accessed through a user-friendly Human Robot Interaction interface. The approach is evaluated on a small parts’ assembly use case deployed onto a collaborative industrial robot testbed. Experiments indicate that the proposed approach allows inexperienced users to efficiently teach robots new assembly tasks.</p>
  </li>
 <li>
    <p><img style="padding: 0 15px; float: left" height="140" src="../images/sarafun.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px" align="justify"></p>
  
 <p align="justify">
  Smart Assembly Robot with Advanced Functionalities (SARAFun), Impact, 2017


</p>
<p align="justify">
While Industrial robots are very successful in many areas of industrial manufacturing, assembly automation still suffers from complex time consuming programming and the need of dedicated hardware. ABB has developed YuMi, a collaborative inherently safe assembly robot that is expected to reduce integration costs significantly by offering a standardized hardware setup and simple fitting of the robot into existing workplaces. Internal Pilot testing at ABB has however shown that when YuMi is programmed with traditional methods the programming time even for simple assembly tasks will remain very long. The SARAFun project has been formed to enable a non-expert user to integrate a new bi-manual assembly task on a YuMi robot in less than a day. This will be accomplished by augmenting the YuMi robot with cutting edge sensory and cognitive abilities as well as reasoning abilities required to plan and execute an assembly task. The overall conceptual approach is that the robot should be capable of learning and executing assembly tasks in a human like manner. Studies will be made to understand how human assembly workers learn and perform assembly tasks. The human performance will be modelled and transferred to the YuMi robot as assembly skills. The robot will learn assembly tasks, such as insertion or folding, by observing the task being performed by a human instructor. The robot will then analyze the task and generate an assembly program, including exception handling, and design 3D printable fingers tailored for gripping the parts at hand. Aided by the human instructor, the robot will finally learn to perform the actual assembly task, relying on sensory feedback from vision, force and tactile sensing as well as physical human robot interaction. During this phase the robot will gradually improve its understanding of the assembly at hand until it is capable of performing the assembly in a fast and robust manner.</p>
  </li>
   <li>
   <p><img style="padding: 0 15px; float: left" height="140" src="../images/nuclear.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px"   align="justify"></p>
  
 <p align="justify">
  Towards Robotic Manipulation for Nuclear Decommissioning: A Pilot Study on Tele-operation and Autonomy
IEEE International Conference on Robotics and Automation for Humanitarian Applications, 2016
 
</p>
<p align="justify">
We present early pilot-studies of a new international project, developing advanced robotics to handle nuclear waste. Despite enormous remote handling requirements, there has been remarkably little use of robots by the nuclear industry. The few robots deployed have been directly teleoperated in rudimentary ways, with no advanced control methods or autonomy. Most remote handling is still done by an aging workforce of highly skilled experts, using 1960s style mechanical Master-Slave devices. In contrast, this paper explores how novice human operators can rapidly learn to control modern robots to perform basic manipulation tasks; also how autonomous robotics techniques can be used for operator assistance, to increase throughput rates, decrease errors, and enhance safety. We compare humans directly teleoperating a robot arm, against human-supervised semi-autonomous control exploiting computer vision, visual servoing and autonomous grasping algorithms. We show how novice operators rapidly improve their performance with training; suggest how training needs might scale with task complexity; and demonstrate how advanced autonomous robotics techniques can help human operators improve their overall task performance. An additional contribution of this paper is to show how rigorous experimental and analytical methods from human factors research, can be applied to perform principled scientific evaluations of human test-subjects controlling robots to perform practical manipulative tasks.</p>
  </li>
  <li>
      <p><img style="padding: 0 15px; float: left" height="140" src="../images/book.png" align=”left”></p>
<p style="margin-top: 10px;"> </p>
<p style="margin-top: 20px" align="justify"></p>
  
 <p align="justify">
  Towards advanced robotic manipulation for nuclear decommissioning, 
      Robots Operating in Hazardous Environments, InTechOpen, 2017
</p>
<p align="justify">
Despite enormous remote handling requirements, remarkably very few robots are being used by the nuclear industry. Most of the remote handling tasks are still performed manually, using conventional mechanical master-slave devices. The few robotic manipulators deployed are directly tele-operated in rudimentary ways, with almost no autonomy or even a pre-programmed motion. In addition, majority of these robots are under-sensored (ie with no proprioception), which prevents them to use for automatic tasks. In this context, primarily this chapter discusses the human operator performance in accomplishing heavy-duty remote handling tasks in hazardous environments such as nuclear decommissioning. Multiple factors are evaluated to analyse the human operators’ performance and workload. Also, direct human tele-operation is compared against human-supervised semi-autonomous control exploiting computer vision. Secondarily, a vision-guided solution towards enabling advanced control and automating the undersensored robots is presented. Maintaining the coherence with real nuclear scenario, the experiments are conducted in the lab environment and results are discussed.</p>
  </li>
</p>



  </div>

</article>

      </div>
    </main>

    <footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <!--h2 class="footer-heading">Dr. Yasemin Bekiroglu</h2-->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
            
            </li>
            
           
                               
         <li>
            <a href="https://scholar.google.com/citations?user=LGgbqWgAAAAJ&hl=en"> 
            <i class="fa fa-google" style="color:gray"></i> google scholar
            </a>
              </li>
          
          <li>
            <!--a href="https://orcid.org/0000-0002-2597-6013 ORCID"-->
            <div itemscope itemtype="https://schema.org/Person"><a itemprop="sameAs" 
  content="https://orcid.org/0000-0002-2597-6013" href="https://orcid.org/0000-0002-2597-6013" 
  target="orcid.widget" rel="me noopener noreferrer" style="vertical-align:top;">
  <img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="width:1em;margin-right:.5em;" 
  alt="ORCID iD icon"></a>
            <!--i class="ai ai-orcid"></i>  </a-->
            <a href="https://www.scopus.com/authid/detail.uri?authorId=36661968000">Scopus</a>
              <a href="https://dblp.org/pers/hd/b/Bekiroglu:Yasemin">
             <i class="ai ai-dblp"></i>
             </a>
            <a href="https://www.linkedin.com/in/yaseminbekiroglu/">
            <i class="fa fa-linkedin"></i>
              </a>
              </div>
            </li>
              <li><i class="fa fa-envelope-o"><a class="u-email" href="mailto:yaseminb@chalmers.se"> yaseminb at chalmers.se</i></a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
